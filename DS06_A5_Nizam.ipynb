{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Reinforcement Learning Implementation\n",
        "\n",
        "***Summary:***\n",
        "\n",
        "The assignment involves **implementing simple reinforcement learning (RL) classes** for an agent and an environment. The agent is a piece of code that aims to gain rewards through interactions, while the environment is an external model of the world providing observations and rewards to the agent. The focus is on creating a basic RL code with a dummy environment that gives random rewards regardless of the agent's actions. The environment class has methods for handling actions, providing observations, checking the end of episodes, and more. The agent class, on the other hand, includes a constructor and a step function allowing the agent to observe the environment, make decisions based on observations, submit actions, and collect rewards.\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "The purpose of the assignment is to provide a hands-on introduction to the implementation of basic reinforcement learning concepts. By creating simple classes for the agent and environment, the assignment aims to help learners understand the fundamental components of RL systems, such as the interaction between the agent and the environment, handling actions and observations, and collecting rewards. The use of a dummy environment with random rewards simplifies the focus on the implementation of the classes, making it accessible for learning purposes without the complexity of practical RL applications."
      ],
      "metadata": {
        "id": "MeKVG7bFROc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SampleEnvironment Class:***\n",
        "\n",
        "* Constructor (__init__): Initializes the environment with a fixed number of steps_left (set to 20 initially).\n",
        "\n",
        "* get_observation Method: Returns a fixed observation, represented by a zero vector [0.0, 0.0, 0.0].\n",
        "\n",
        "* get_actions Method: Returns a list of available actions, [0, 1].\n",
        "\n",
        "* is_done Method: Checks if the number of remaining steps is zero, indicating the end of the episode.\n",
        "\n",
        "* action Method: Accepts an action as an argument, decrements the steps_left counter, and returns a random reward using random.choice([0, 1]).\n",
        "\n",
        "***Agent Class:***\n",
        "\n",
        "* Constructor (__init__): Initializes the agent with an initial total_reward set to zero.\n",
        "\n",
        "* step Method: Accepts an instance of the SampleEnvironment class. It obtains the current observation from the environment, prints it, gets the available actions, selects a random action, performs the action in the environment, and updates the agent's total_reward with the obtained reward.\n",
        "\n",
        "***Main Execution (__main__ block):***\n",
        "\n",
        "* Creates an instance of SampleEnvironment and Agent.\n",
        "\n",
        "* Enters a loop until the environment is done.\n",
        "\n",
        "* In each iteration, the agent takes a step in the environment by calling the step method.\n",
        "\n",
        "* Prints the total reward obtained by the agent at the end of the episode.\n",
        "\n",
        "\n",
        "Note: The code currently has a small issue in the action method where it attempts to return both random.random() and random.choice([0,1]). The corrected line should be return random.choice([0, 1]). Additionally, the provided code includes some unnecessary line breaks and indentation inconsistencies, which might be cleaned up for better readability."
      ],
      "metadata": {
        "id": "vyeymlQtV0DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "class SampleEnvironment:\n",
        "    def __init__(self):\n",
        "        self.steps_left = 20\n",
        "\n",
        "    def get_observation(self) -> List[float]:\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\n",
        "    def get_actions(self) -> List[int]:\n",
        "        return [0, 1]\n",
        "\n",
        "    def is_done(self) -> bool:\n",
        "        return self.steps_left == 0\n",
        "\n",
        "    def action(self, action: int) -> float:\n",
        "        if self.is_done():\n",
        "            raise Exception(\"Game is over\")\n",
        "        self.steps_left -= 1\n",
        "        return random.random()"
      ],
      "metadata": {
        "id": "qIT-vt0eXgoz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent's Class simple and includes only two methods: the constructor and the method that performs one step in the environment\n",
        "\n",
        "Intitially the total reward collected is set to zero by the constructor.\n",
        "\n",
        "The step function accepts environment instance as an argument and allows agent to perform the following actions:\n",
        "\n",
        "* Observe the environment\n",
        "* Make a decision about the action to take based on the observations\n",
        "* Submit the action to the environment\n",
        "* Get the reward for the current step"
      ],
      "metadata": {
        "id": "vteTE-dAXwlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.choice([0,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnobW8nRYf6n",
        "outputId": "19d0d318-2580-4a10-8da2-8d1ccf1ecd20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step(self, env: SampleEnvironment):\n",
        "        current_obs = env.get_observation()\n",
        "        print(\"Observation {}\".format(current_obs))\n",
        "        actions = env.get_actions()\n",
        "        print(actions)\n",
        "        reward = env.action(random.choice(actions))\n",
        "        self.total_reward += reward\n",
        "        print(\"Total Reward {}\".format(self.total_reward))"
      ],
      "metadata": {
        "id": "7qeBM5OvYj7B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = SampleEnvironment()\n",
        "    agent = Agent()\n",
        "    i=0\n",
        "\n",
        "    while not env.is_done():\n",
        "        i=i+1\n",
        "        print(\"Steps {}\".format(i))\n",
        "        agent.step(env)\n",
        "\n",
        "    print(\"Total reward got: %.4f\" % agent.total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ldp0lo2YrAa",
        "outputId": "1c95cf52-3339-4593-9b15-3327bd36d7c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps 1\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.7871281417402183\n",
            "Steps 2\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.8556236780426499\n",
            "Steps 3\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.8511079531670174\n",
            "Steps 4\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.9353772791736956\n",
            "Steps 5\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 2.784935841401925\n",
            "Steps 6\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.5213220934236906\n",
            "Steps 7\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.1846044640479\n",
            "Steps 8\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.350056926015854\n",
            "Steps 9\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.909398134970374\n",
            "Steps 10\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 5.66861173350634\n",
            "Steps 11\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.27447420231237\n",
            "Steps 12\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.347758195551178\n",
            "Steps 13\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.40738997642766\n",
            "Steps 14\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.673484842242215\n",
            "Steps 15\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.221153466182532\n",
            "Steps 16\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.661213357471579\n",
            "Steps 17\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 8.397869192949228\n",
            "Steps 18\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 8.590238011007457\n",
            "Steps 19\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 9.563325356475996\n",
            "Steps 20\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 10.414626985821245\n",
            "Total reward got: 10.4146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Take Note! with current code this the result:**\n",
        "\n",
        "Here's a breakdown of the results:\n",
        "\n",
        "* Observation: The environment's observation is a fixed zero vector [0.0, 0.0, 0.0].\n",
        "\n",
        "* Available Actions: The environment provides two available actions, [0, 1].\n",
        "\n",
        "* Agent's Steps: In each step, the agent:\n",
        "\n",
        "1.   Obtains the current observation.\n",
        "2.   Prints the available actions.\n",
        "3. Chooses a random action from the available actions.\n",
        "4. Performs the chosen action in the environment.\n",
        "5. Updates the agent's total_reward with the obtained reward.\n",
        "\n",
        "* Total Reward: The total reward obtained by the agent after 20 steps is printed as 10.4146\n",
        "\n",
        "The results indicate that the agent is interacting with the environment, making random decisions, and accumulating rewards based on the dummy environment's logic. This demonstrates a basic structure for implementing reinforcement learning, where an agent interacts with an environment, makes decisions, and learns from the rewards received.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dyc2vuqaZJTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To improve the code, we can consider several enhancements and modifications. Here are a few suggestions:***\n",
        "\n",
        "These changes introduce a new environment class, **CustomEnvironment**, with an additional method **custom_action** that provides rewards based on a combination of a base reward and a bonus reward depending on the chosen action. The agent class, **ModifiedAgent**, is modified to consider these changes in the decision-making process. The main program is updated to use the new environment and agent classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "P14m6pZJgAvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***New Environment Class: CustomEnvironment***\n",
        "\n",
        "\n",
        "* a) Create a new environment class called CustomEnvironment.\n",
        "\n",
        "\n",
        "* b) Modify the __init__ method to include a parameter for the initial number of steps, which can vary for different instances of the environment.\n",
        "\n",
        "\n",
        "* c) Implement a new method called custom_action that takes an action as input and returns a reward. This time, the reward should be a combination of a base reward and a bonus reward based on the action. For example, if the action is 0, the bonus reward could be 0.2, and if the action is 1, the bonus reward could be 0.5.\n",
        "\n",
        "**In this version of the CustomEnvironment class:**\n",
        "\n",
        "* **The __init__ **method now accepts an additional parameter **initial_steps** to set the initial number of steps.\n",
        "\n",
        "* The **custom_action** method has been implemented as requested, providing a reward based on a combination of a base reward and a bonus reward depending on the chosen action.\n",
        "\n",
        "use this **CustomEnvironment class** as a replacement for the **SampleEnvironment** class in your code.\n"
      ],
      "metadata": {
        "id": "2PyIwCNohsRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "class CustomEnvironment:\n",
        "    def __init__(self, initial_steps: int):\n",
        "        self.steps_left = initial_steps\n",
        "\n",
        "    def get_observation(self) -> List[float]:\n",
        "        # Implement a dynamic observation (e.g., a random vector)\n",
        "        return [random.random() for _ in range(3)]\n",
        "\n",
        "    def get_actions(self) -> List[int]:\n",
        "        return [0, 1]\n",
        "\n",
        "    def is_done(self) -> bool:\n",
        "        return self.steps_left == 0\n",
        "\n",
        "    def custom_action(self, agent_action: int) -> float:\n",
        "        if self.is_done():\n",
        "            raise Exception(\"Game is over\")\n",
        "        self.steps_left -= 1\n",
        "\n",
        "        # Implement a reward mechanism based on the chosen action\n",
        "        base_reward = random.uniform(0.0, 1.0)\n",
        "        bonus_reward = 0.2 if agent_action == 0 else 0.5\n",
        "\n",
        "        return base_reward + bonus_reward\n"
      ],
      "metadata": {
        "id": "F55kHiBgh7EE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent Class Modification: ModifiedAgent**\n",
        "\n",
        "* a) Create a new agent class called ModifiedAgent.\n",
        "\n",
        "* b) Modify the step method to incorporate the changes in the CustomEnvironment. Instead of using random.choice(actions), the agent should now decide between the available actions based on the bonus rewards provided by the environment.\n",
        "\n",
        "In this version of the **ModifiedAgent** class:\n",
        "\n",
        "* The step method now accepts an instance of **CustomEnvironment** instead of **SampleEnvironment**.\n",
        "\n",
        "* The **choose_action** method has been added to allow the agent to make decisions based on both available actions and observations from the environment.\n",
        "\n",
        "* The decision-making strategy is modified based on the bonus rewards and observations. You can customize this strategy according to your requirements.\n",
        "\n",
        " use this **ModifiedAgent** class as a replacement for the **Agent** class in your code."
      ],
      "metadata": {
        "id": "QZThUGGpj09A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedAgent:\n",
        "    def __init__(self):\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step(self, env: CustomEnvironment):\n",
        "        current_obs = env.get_observation()\n",
        "        print(\"Observation {}\".format(current_obs))\n",
        "        actions = env.get_actions()\n",
        "\n",
        "        # Modify the decision-making process based on bonus rewards\n",
        "        chosen_action = self.choose_action(actions, current_obs)\n",
        "\n",
        "        reward = env.custom_action(chosen_action)\n",
        "        self.total_reward += reward\n",
        "        print(\"Chosen Action: {}, Total Reward {:.4f}\".format(chosen_action, self.total_reward))\n",
        "\n",
        "    def choose_action(self, actions: List[int], observation: List[float]) -> int:\n",
        "        # Modify the decision-making strategy based on bonus rewards and observation\n",
        "        # For example, choose action 0 if the first observation value is less than 0.5, otherwise choose action 1\n",
        "        return 0 if observation[0] < 0.5 else 1\n"
      ],
      "metadata": {
        "id": "qcrsK2Bgk60W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Program Modification:**\n",
        "\n",
        "* a) Change the instantiation of the environment to use the new CustomEnvironment class.\n",
        "\n",
        "* b) Update the agent instantiation to use the ModifiedAgent class.\n",
        "\n",
        "In this version:\n",
        "\n",
        "* The instantiation of the environment is changed to use the new **CustomEnvironment** class (**custom_env = CustomEnvironment(initial_steps=20**)).\n",
        "\n",
        "* The agent instantiation is updated to use the **ModifiedAgent** class (**modified_agent = ModifiedAgent**())."
      ],
      "metadata": {
        "id": "iIfbCm7TlABZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    custom_env = CustomEnvironment(initial_steps=20)\n",
        "    modified_agent = ModifiedAgent()\n",
        "    i = 0\n",
        "\n",
        "    while not custom_env.is_done():\n",
        "        i += 1\n",
        "        print(\"Steps {}\".format(i))\n",
        "        modified_agent.step(custom_env)\n",
        "\n",
        "    print(\"Total reward obtained: {:.4f}\".format(modified_agent.total_reward))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuLTMXW7mBcu",
        "outputId": "506d16bc-ad8b-4347-abe7-fe295f1e02fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps 1\n",
            "Observation [0.5974081739106338, 0.13791546300113056, 0.55652214647664]\n",
            "Chosen Action: 1, Total Reward 1.2911\n",
            "Steps 2\n",
            "Observation [0.5767904865520905, 0.6060165558667303, 0.8768420191085013]\n",
            "Chosen Action: 1, Total Reward 2.7610\n",
            "Steps 3\n",
            "Observation [0.69334693284241, 0.7494663759537192, 0.1522173046974986]\n",
            "Chosen Action: 1, Total Reward 3.9129\n",
            "Steps 4\n",
            "Observation [0.3813915525391739, 0.33007615796303, 0.9240473224669049]\n",
            "Chosen Action: 0, Total Reward 4.2024\n",
            "Steps 5\n",
            "Observation [0.9222741532563912, 0.8546009067256808, 0.600385354881331]\n",
            "Chosen Action: 1, Total Reward 5.1156\n",
            "Steps 6\n",
            "Observation [0.9372295368410759, 0.7272092603491358, 0.6440083797970427]\n",
            "Chosen Action: 1, Total Reward 5.7128\n",
            "Steps 7\n",
            "Observation [0.6015965877733098, 0.19180200821671411, 0.13190095438946214]\n",
            "Chosen Action: 1, Total Reward 6.8903\n",
            "Steps 8\n",
            "Observation [0.18012001515658282, 0.36402130878556493, 0.4510674754512063]\n",
            "Chosen Action: 0, Total Reward 7.5530\n",
            "Steps 9\n",
            "Observation [0.48245605619107423, 0.28429050116576604, 0.03289174140470985]\n",
            "Chosen Action: 0, Total Reward 8.6330\n",
            "Steps 10\n",
            "Observation [0.31524683112549656, 0.9567903426957607, 0.980306879603201]\n",
            "Chosen Action: 0, Total Reward 9.6897\n",
            "Steps 11\n",
            "Observation [0.9829858166477411, 0.869038182882176, 0.6072605517559282]\n",
            "Chosen Action: 1, Total Reward 11.0752\n",
            "Steps 12\n",
            "Observation [0.4441602184031599, 0.03400326916566554, 0.7718675839078121]\n",
            "Chosen Action: 0, Total Reward 11.7589\n",
            "Steps 13\n",
            "Observation [0.8169602324124874, 0.47816942230950654, 0.4235343873990358]\n",
            "Chosen Action: 1, Total Reward 13.1360\n",
            "Steps 14\n",
            "Observation [0.1702430933892295, 0.30352561423528557, 0.10313160371523267]\n",
            "Chosen Action: 0, Total Reward 14.1043\n",
            "Steps 15\n",
            "Observation [0.5935032633262443, 0.22459026938362814, 0.44602629999356]\n",
            "Chosen Action: 1, Total Reward 15.3212\n",
            "Steps 16\n",
            "Observation [0.6501211368893358, 0.5787062613366473, 0.7158243418634084]\n",
            "Chosen Action: 1, Total Reward 16.6583\n",
            "Steps 17\n",
            "Observation [0.32076601164416707, 0.007316395616933535, 0.9776176562596961]\n",
            "Chosen Action: 0, Total Reward 17.0479\n",
            "Steps 18\n",
            "Observation [0.027865468650687064, 0.11226676440200112, 0.07555369016575997]\n",
            "Chosen Action: 0, Total Reward 18.0052\n",
            "Steps 19\n",
            "Observation [0.42901144140662417, 0.7303399377412074, 0.6100206030898827]\n",
            "Chosen Action: 0, Total Reward 18.9827\n",
            "Steps 20\n",
            "Observation [0.2029276447144227, 0.09380911037005768, 0.9654067152851554]\n",
            "Chosen Action: 0, Total Reward 19.5162\n",
            "Total reward obtained: 19.5162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided output represents the steps and results obtained during the execution of the modified code. Let's break down the key information:\n",
        "\n",
        "* **Steps 1 to 20:** Each step represents an interaction between the agent and the environment.\n",
        "* **Observation:** The environment generates a dynamic observation at each step, represented as a list of three random values.\n",
        "* **Chosen Action:** The agent makes decisions based on the custom decision-making strategy implemented in the ModifiedAgent class. In this case, the agent chooses between actions 0 and 1 based on the first value of the observation. If the first observation value is less than 0.5, the agent chooses action 0; otherwise, it chooses action 1.\n",
        "* **Total Reward:** The total reward obtained by the agent is accumulated at each step. The reward is calculated based on the custom_action method in the CustomEnvironment class, which combines a base reward and a bonus reward based on the chosen action.\n",
        "* **Total reward obtained:** The final total reward obtained by the agent after 20 steps is printed as **19.5162.**\n",
        "\n",
        "The output demonstrates the interaction between the modified agent and the custom environment, where the agent's decision-making is influenced by the observations provided by the environment. The dynamic nature of the observations and the bonus rewards contribute to the variability in the agent's actions and the total reward obtained."
      ],
      "metadata": {
        "id": "ZCcmH5U_maix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conclusion\n",
        "\n",
        "The point of this exercise is to demonstrate the process of making improvements to a basic reinforcement learning setup. By introducing a more complex environment (**CustomEnvironment**) and modifying the agent's decision-making strategy (**ModifiedAgent**), we aim to enhance the learning capabilities and adaptability of the agent.\n",
        "\n",
        "***Comparison with the Original Result:***\n",
        "\n",
        "* **Dynamic Environment:** In the modified version, the environment provides dynamic observations, making it more realistic compared to the fixed observation [0.0, 0.0, 0.0] in the original version. This allows the agent to learn from different states of the environment.\n",
        "\n",
        "* **Custom Action and Bonus Rewards:** The modified environment introduces a custom_action method that combines base rewards with bonus rewards based on the chosen action. This provides a more sophisticated reward structure compared to the random rewards in the original version, potentially allowing the agent to learn more meaningful behaviors\n",
        "\n",
        "* **Modified Decision-Making:** The modified agent class (ModifiedAgent) incorporates observations into the decision-making process, allowing the agent to adapt its actions based on the dynamic environment. In contrast, the original agent class (Agent) made random decisions without considering the environment's state.\n",
        "\n",
        "* **Learning Strategy:** In the modified version, the agent's decision to choose action 0 or 1 is influenced by the first value of the observation. This introduces a basic form of learning, where the agent associates observations with actions, potentially leading to more informed decisions over time.\n",
        "\n",
        "***Potential Areas for Further Improvement:***\n",
        "\n",
        "* **Advanced Learning Algorithms:** Consider implementing more advanced learning algorithms such as Q-learning or deep reinforcement learning techniques to enhance the agent's learning capabilities.\n",
        "\n",
        "* **Tuning Parameters:** Experiment with different parameter values, such as bonus reward magnitudes, learning rates, and exploration-exploitation trade-offs, to fine-tune the agent's behavior.\n",
        "\n",
        "* **Complex Observations:** Introduce more complex observations to challenge the agent and require it to learn more intricate strategies.\n",
        "\n",
        "* **Evaluation Metrics:** Define and use appropriate evaluation metrics to assess the performance of the agent over multiple episodes or environments.\n",
        "\n",
        "Overall, the exercise provides a foundation for understanding how modifications to the environment and agent can impact the learning process in reinforcement learning. The improvements aim to make the learning environment more realistic and the agent more adaptive to different situations."
      ],
      "metadata": {
        "id": "UHpWR5M-nDSm"
      }
    }
  ]
}